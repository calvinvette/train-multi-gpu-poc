# SPDX-License-Identifier: AGPL-3.0-only
# Copyright (c) 2025 Calvin Vette
# Dockerfile.flash-attention.amd64
# Compile flash-attention with certain minimal GPU architecture levels
# (NVIDIA Compute Capability)
# Use Flash Attention 2.8.3 for Orin/A100 (Amphere)
# Use Flash Attention 3.x for Thor/H[100,H200]/[G]B200 (Blackwell)
# Compile for x86/amd64 for [H,B,GB][100,200]
# Compile for ARM64 for Orin/Thor AGX
# export as a whl to /out

# ---- builder ----
FROM docker.io/nvidia/cuda:12.6.0-devel-ubuntu22.04

RUN mkdir -p /workspace
WORKDIR /workspace

RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y \
    git build-essential gcc make cmake ninja-build libopenmpi-dev openmpi-bin 

RUN DEBIAN_FRONTEND=noninteractive apt-get install -y \
	software-properties-common \
	apt-transport-https \
	curl \
	wget

RUN add-apt-repository ppa:deadsnakes/ppa

# Upgrade pip and install uv
RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y python3.12 
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.12 6
RUN apt-get remove -y python3.10
RUN apt-get install -y python3.12-venv python3.12-dev
RUN /usr/bin/python3.12 -m ensurepip --upgrade
RUN pip3 install --no-cache-dir --upgrade pip
RUN pip3 install packaging uv wheel

# First install torch and friends (necessary for the build)
# PyTorch + CUDA 12.6 wheel (smaller than NGC PyTorch image)
#RUN uv pip install --system --no-cache-dir --index-url https://download.pytorch.org/whl/cu126 \
RUN pip install --index-url https://download.pytorch.org/whl/cu126 \
    torch \
    torchvision \
    torchaudio

# Make sure /out is volume-mapped because this is where we'll dump the whl when we're done
RUN mkdir -p /out

# Compile Flash Attention
# TODO - Parameterize the version and file name to make upgrades easier
# TODO - optimize this build (see NGE/torch-cuda/build.sh)
# TODO - migrate this out to its own build job and deposit into local pip repo
#   (See NGE's build system)
# Doing the straight pip install does the (unoptimized compile)
# We're paying a penalty for this regardless
# RUN uv pip install --system flash-attn==2.8.3 --no-build-isolation
# First build flash_attention
# TODO - Parameterize the GPU Arch
# Build for Ampere and above; Consider flash-attn3 for Blackwell
ENV SMS="90"            
ENV CUDA_ARCH_BIN="90"
ENV TORCH_CUDA_ARCH_LIST="9.0"
ENV CUDA_HOME="/usr/local/cuda"
# TODO - Verify these optimizations; they change over time
# We need to build MAGMA (BLAS) with CUDA support
ENV MAGMA_ENABLE_CUDA=ON
# Pytorch build recommends this. Why?
ENV USE_PRIORITIZED_TEXT_FOR_LD=1
# Use the new C++ ABI
ENV _GLIBCXX_USE_CXX11_ABI=1
# CUDNN
ENV USE_SYSTEM_CUDNN=1


# Now let's actually build the thing
RUN git clone https://github.com/Dao-AILab/flash-attention.git

# Add MAX_JOBS=4 before the python3 setup.py if we have 96GB or RAM or less
# RUN cd flash-attention && MAX_JOBS=8 python3 setup.py build develop bdist_wheel
# Each job maxes out around 15GB of RAM, and each job will also max out around 4 threads
# The build will likely crash if we go over RAM, but will just be slower if we go over threads
# So we'll calculate the lesser of the two for max parallelization in the build
# YMMV, but this is the closest I've come to making this systematic as oppose to experiential
# max_jobs=$(mem_jobs=$(expr $(free --giga | grep Mem: | awk '{print $2}') / 15); proc_jobs=$(expr $(nproc) / 4); echo $(($proc_jobs < $mem_jobs ? $proc_jobs : $mem_jobs)))
#RUN cd flash-attention && MAX_JOBS=$(expr $(nproc) / 4) python3.12 setup.py build develop bdist_wheel
RUN cd flash-attention/hopper && MAX_JOBS=$(mem_jobs=$(expr $(free --giga | grep Mem: | awk '{print $2}') / 15); proc_jobs=$(expr $(nproc) / 4); echo $(($proc_jobs < $mem_jobs ? $proc_jobs : $mem_jobs))) python3.12 setup.py build develop bdist_wheel

# Wheel should be here:
# /workspace/flash-attention/dist/flash_attn-2.8.3-cp310-cp310-linux_x86_64.whl
# Run this container and dump it out:
# mkdir -p ../out
# docker run \
#         --rm \
#         -it \
#         -v ../out:/out \
#         --name booger nge/builder.flash-attention \
#         cp /workspace/flash-attention/dist/flash_attn-2.8.3-cp310-cp310-linux_x86_64.whl /out/
